{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c7bc08-dbe4-4fa3-9577-3cfe8f945ef4",
   "metadata": {},
   "source": [
    "# Geospatial Data Science Tutorial for IC2S2'23\n",
    "# 2. SPATIAL STATISTICS: Choropleth maps, Spatial autocorrelation\n",
    "\n",
    "This notebook gives an introduction of geospatial data science basics to computational social scientists, covering:\n",
    "- Choropleth maps\n",
    "- Spatial autocorrelation\n",
    "\n",
    "This notebook was adapted from:\n",
    "* A course on Geographic Data Science: https://darribas.org/gds_course/content/bD/lab_D.html\n",
    "* A course on Geographic Data Science: https://darribas.org/gds_course/content/bE/lab_E.html\n",
    "* A course on Geographic Data Science: https://darribas.org/gds_course/content/bF/lab_F.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fbd264-ddf3-4402-ab2d-cf71f30afdf8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b3000-cba8-412e-9925-3962710c6f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0' # Temporary fix to use the brand-new shapely 2.0\n",
    "\n",
    "import geopandas as gpd # for geospatial data handling\n",
    "import pandas as pd\n",
    "import contextily as cx # for plotting\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import seaborn as sns # for plotting\n",
    "\n",
    "# pysal and esda methods for choropleths and spatial autocorrelation\n",
    "from pysal.lib import examples\n",
    "from pysal.viz import mapclassify\n",
    "from pysal.lib import weights\n",
    "from libpysal.io import open as psopen\n",
    "from splot.esda import (\n",
    "    moran_scatterplot, lisa_cluster, plot_local_autocorrelation, plot_moran\n",
    ")\n",
    "from splot.libpysal import plot_spatial_weights\n",
    "import esda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1fc263-71c0-446b-a015-f056341ceb07",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We use the [Mexico GDP per capita dataset](https://geographicdata.science/book/data/mexico/README.html), which we can access as a PySAL example dataset. You can read more about PySAL example datasets [here](https://pysal.org/libpysal/notebooks/examples.html).\n",
    "\n",
    "We can get a short explanation of the dataset through the `explain` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a9765-ddd8-4762-b89f-4bb68f27524e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples.explain(\"mexico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa1b2c3-ffd0-4eeb-af54-b7d912059bb8",
   "metadata": {},
   "source": [
    "Now, to download it from its remote location, we can use `load_example`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e8a38-91ea-40ee-89ad-da6bca8008c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mx = examples.load_example(\"mexico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3089f0-799d-497b-bede-aab0610a10fd",
   "metadata": {},
   "source": [
    "This will download the data and place it on your home directory. We can inspect the directory where it is stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d80ff-ea09-4928-9fe4-6f884c48f341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mx.get_file_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4ce5a-e4d2-41ee-92dc-deaa27870fff",
   "metadata": {},
   "source": [
    "For this section, we will read the ESRI shapefile, which we can do by pointing `geopandas.read_file` to the `.shp` file. The utility function `get_path` makes it a bit easier for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d9e12-bccf-490f-b080-e0961c89c432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = gpd.read_file(examples.get_path(\"mexicojoin.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877510e8-535b-485f-92a9-df1ec6c417b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad7609a-c3de-4b6b-80af-352fcdf43300",
   "metadata": {},
   "source": [
    "And, from now on, `db` is a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5ddcd-7ab3-436a-9978-c7a4cb09fbed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28619bb1-d7ff-4687-bdb6-d9596b8ce17c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bb5a97-5796-41e2-94d5-3f92eb1ac143",
   "metadata": {},
   "source": [
    "The data however does not include a CRS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15339e11-f27a-4449-81f8-a2e0fe632d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dcc16f-d9ee-40cd-90f1-0c7074b0e050",
   "metadata": {},
   "source": [
    "To be able to add baselayers, we need to specify one. Looking at the details and the original reference, we find the data are expressed in longitude and latitude, so the CRS we can use is `EPSG:4326`. Let's add it to `db`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad07d66f-f069-4ec8-bcf7-79989c122979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.crs = \"EPSG:4326\"\n",
    "db.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6770d1-dae1-43bc-bf0c-fcbb2483c900",
   "metadata": {},
   "source": [
    "Now we are fully ready to map!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeecc3d-a564-4cd3-a678-f1cb3b4aa853",
   "metadata": {},
   "source": [
    "### Unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5607695-51de-41d9-87dd-9f9741c41831",
   "metadata": {},
   "source": [
    "A choropleth for categorical variables simply assigns a different color to every potential value in the series. The main requirement in this case is then for the color scheme to reflect the fact that different values are not ordered or follow a particular scale.\n",
    "\n",
    "In Python, creating categorical choropleths is possible with one line of code. To demonstrate this, we can plot the Mexican states and the region each belongs to based on the Mexican Statistics Institute (coded in our table as the `INEGI` variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2774c67e-94a5-4c6f-92c4-542055ec6a85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.plot(\n",
    "    column=\"INEGI\", \n",
    "    categorical=True, \n",
    "    legend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2155185-4eb0-4a63-87b9-1d4eda8c8f3e",
   "metadata": {},
   "source": [
    "Let us stop for a second in a few crucial aspects:\n",
    "\n",
    "* Note how we are using the same approach as for basic maps, the command `plot`, but we now need to add the argument `column` to specify which column in particular is to be represented.\n",
    "* Since the variable is categorical we need to make that explicit by setting the argument `categorical` to `True`.\n",
    "* As an optional argument, we can set `legend` to `True` and the resulting figure will include a legend with the names of all the values in the map.\n",
    "* Unless we specify a different colormap, the selected one respects the categorical nature of the data by not implying a gradient or scale but a qualitative structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5fa0c7-b360-43c6-87ec-6a718e172964",
   "metadata": {},
   "source": [
    "### Equal interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd9c688-5966-4bd2-8d78-e710b35adf0b",
   "metadata": {},
   "source": [
    "If, instead of categorical variables, we want to display the geographical distribution of a continuous phenomenon, we need to select a way to encode each value into a color. One potential solution is applying what is usually called \"equal intervals\". The intuition of this method is to split the *range* of the distribution, the difference between the minimum and maximum value, into equally large segments and to assign a different color to each of them according to a palette that reflects the fact that values are ordered.\n",
    "\n",
    "Creating the choropleth is relatively straightforward in Python. For example, to create an equal interval on the GDP per capita in 2000 (`PCGDP2000`), we can run a similar command as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108bd02-cd44-4270-9980-3c3c9e4d9ada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"equal_interval\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4e988-f427-4dd7-865f-c32b57ddaff9",
   "metadata": {},
   "source": [
    "Pay attention to the key differences:\n",
    "\n",
    "* Instead of specifyig `categorical` as `True`, we replace it by the argument `scheme`, which we will use for all choropleths that require a continuous classification scheme. In this case, we set it to `equal_interval`.\n",
    "* As above, we set the number of colors to 7. Note that we need not pass the bins we calculated above, the plotting method does it itself under the hood for us.\n",
    "* As optional arguments, we can change the colormap to a yellow to green gradient, which is one of the recommended ones by [ColorBrewer](http://colorbrewer2.org/) for a sequential palette. \n",
    "\n",
    "Now, let's dig a bit deeper into the classification, and how exactly we are encoding values into colors. Each segment, also called bins or buckets, can also be calculated using the library `mapclassify` from the `PySAL` family:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd21c89d-ea7d-4b5e-af10-ed45d198a2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classi = mapclassify.EqualInterval(db[\"PCGDP2000\"], k=7)\n",
    "classi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751d60e-ced2-4ef1-9ff5-c47a6ba2e0a0",
   "metadata": {},
   "source": [
    "The only additional argument to pass to `Equal_Interval`, other than the actual variable we would like to classify is the number of segments we want to create, `k`, which we are arbitrarily setting to seven in this case. This will be the number of colors that will be plotted on the map so, although having several can give more detail, at some point the marginal value of an additional one is fairly limited, given the ability of the brain to tell any differences.\n",
    "\n",
    "Once we have classified the variable, we can check the actual break points where values stop being in one class and become part of the next one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e58f5-f866-44e5-947e-c0f4e85d1f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classi.bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23a01c-fdf3-4206-8697-46e24db96296",
   "metadata": {},
   "source": [
    "The array of breaking points above implies that any value in the variable below 15,207.57 will get the first color in the gradient when mapped, values between 15,207.57 and 21,731.14 the next one, and so on.\n",
    "\n",
    "The key characteristic in equal interval maps is that the bins are allocated based on the magnitude on the values, irrespective of how many obervations fall into each bin as a result of it. In highly skewed distributions, this can result in bins with a large number of observations, while others only have a handful of outliers. This can be seen in the summary table printed out above, where ten states are in the first group, but only one of them belong to the one with highest values. This can also be represented visually with a kernel density plot where the break points are included as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3289d43-ad7f-4478-8181-d9a1e147b715",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "f, ax = plt.subplots(1)\n",
    "# Plot the kernel density estimation (KDE)\n",
    "sns.kdeplot(db[\"PCGDP2000\"], fill=True)\n",
    "# Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "sns.rugplot(db[\"PCGDP2000\"], alpha=0.5)\n",
    "# Loop over each break point and plot a vertical red line\n",
    "for cut in classi.bins:\n",
    "    plt.axvline(cut, color='red', linewidth=0.75)\n",
    "# Display image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc009ad6-060b-4345-b4e8-311537b6fe91",
   "metadata": {},
   "source": [
    "Technically speaking, the figure is created by overlaying a KDE plot with vertical bars for each of the break points. This makes much more explicit the issue highlighed by which the first bin contains a large amount of observations while the one with top values only encompasses a handful of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d57fe22-17bb-4fc8-8939-713a69d5c9fa",
   "metadata": {},
   "source": [
    "### Quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e780b8-862b-419e-a422-985ca51fa60f",
   "metadata": {},
   "source": [
    "One solution to obtain a more balanced classification scheme is using quantiles. This, by definition, assigns the same amount of values to each bin: the entire series is laid out in order and break points are assigned in a way that leaves exactly the same amount of observations between each of them. This \"observation-based\" approach contrasts with the \"value-based\" method of equal intervals and, although it can obscure the magnitude of extreme values, it can be more informative in cases with skewed distributions.\n",
    "\n",
    "The code required to create the choropleth mirrors that needed above for equal intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb0015a-ba00-44e2-9d66-48c03586dab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"quantiles\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc19f3c-46cb-4065-8581-83018b4c57ba",
   "metadata": {},
   "source": [
    "Note how, in this case, the amount of polygons in each color is by definition much more balanced (almost equal in fact, except for rounding differences). This obscures outlier values, which get blurred by significantly smaller values in the same group, but allows to get more detail in the \"most populated\" part of the distribution, where instead of only white polygons, we can now discern more variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a586e12-1b6a-4f18-8c74-98037f166b39",
   "metadata": {},
   "source": [
    "To get further insight into the quantile classification, let's calculate it with `mapclassify`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b0b8f-6b33-437e-a2d4-056dda6a635d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classi = mapclassify.Quantiles(db[\"PCGDP2000\"], k=4)\n",
    "classi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d0b5c-63ba-4699-ba5a-79fe84aaa900",
   "metadata": {},
   "source": [
    "And, similarly, the bins can also be inspected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df8c63-3c37-498d-8dab-647bbc5df678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classi.bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226c0e21-f84b-4d6a-851e-e1251c267b84",
   "metadata": {},
   "source": [
    "The visualization of the distribution can be generated in a similar way as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe0878-4c02-4488-b088-b122eff05e04",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "f, ax = plt.subplots(1)\n",
    "# Plot the kernel density estimation (KDE)\n",
    "sns.kdeplot(db[\"PCGDP2000\"], fill=True)\n",
    "# Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "sns.rugplot(db[\"PCGDP2000\"], alpha=0.5)\n",
    "# Loop over each break point and plot a vertical red line\n",
    "for cut in classi.bins:\n",
    "    plt.axvline(cut, color='red', linewidth=0.75)\n",
    "# Display image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd551c34-1d71-4ed8-8956-2a40bd16a8bc",
   "metadata": {},
   "source": [
    "### Fisher-Jenks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc7bc0-02eb-4426-bf02-89f8d36be69a",
   "metadata": {},
   "source": [
    "Equal interval and quantiles are only two examples of very many classification schemes to encode values into colors. Although not all of them are integrated into `geopandas`, `PySAL` includes several other classification schemes (for a detailed list, have a look at this [link](https://pysal.org/mapclassify/notebooks/01_maximum_breaks.html)). As an example of a more sophisticated one, let us create a Fisher-Jenks choropleth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15788037-95e8-42df-846c-6f4030a00390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"fisher_jenks\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc8311-52e6-4c11-8414-d8b6e3bb389b",
   "metadata": {},
   "source": [
    "The same classification can be obtained with a similar approach as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded735c-9368-48e9-9ad0-c54c3bfe8cd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classi = mapclassify.FisherJenks(db[\"PCGDP2000\"], k=7)\n",
    "classi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d6338-69d4-4699-8748-022e2702018d",
   "metadata": {},
   "source": [
    "This methodology aims at minimizing the variance *within* each bin while maximizing that *between* different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25616b7a-68c0-4698-888f-4b598da49f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classi.bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e4880-04a5-476b-9b82-57a87762a78e",
   "metadata": {},
   "source": [
    "Graphically, we can see how the break points are not equally spaced but are adapting to obtain an optimal grouping of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7343688-52f0-4ac8-b7c5-0e51ed598f71",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "f, ax = plt.subplots(1)\n",
    "# Plot the kernel density estimation (KDE)\n",
    "sns.kdeplot(db[\"PCGDP2000\"], fill=True)\n",
    "# Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "sns.rugplot(db[\"PCGDP2000\"], alpha=0.5)\n",
    "# Loop over each break point and plot a vertical red line\n",
    "for cut in classi.bins:\n",
    "    plt.axvline(cut, color='red', linewidth=0.75)\n",
    "# Display image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427650ec-5d47-4242-8f42-b459dc28b3a5",
   "metadata": {},
   "source": [
    "For example, the bin with highest values covers a much wider span that the one with lowest, because there are fewer states in that value range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d71797-94ff-4af4-8ad6-aef27c58514e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Back to presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9230bf-145d-4dc4-aee0-c710257d95f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bed6db-ebad-44d1-8dfd-f9c294921e8a",
   "metadata": {},
   "source": [
    "# Spatial autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d64e899-213f-439d-a9fd-ddc2ff193fb5",
   "metadata": {},
   "source": [
    "In this session we will be learning the ins and outs of one of the key pieces in spatial analysis: spatial weights matrices. These are structured sets of numbers that formalize geographical relationships between the observations in a dataset. Essentially, a spatial weights matrix of a given geography is a positive definite matrix of dimensions $N$ by $N$, where $N$ is the total number of observations:\n",
    "\n",
    "$\n",
    "W = \\left(\\begin{array}{cccc}\n",
    "0 & w_{12} & \\dots & w_{1N} \\\\\n",
    "w_{21} & \\ddots & w_{ij} & \\vdots \\\\\n",
    "\\vdots & w_{ji} & 0 & \\vdots \\\\\n",
    "w_{N1} & \\dots & \\dots & 0 \n",
    "\\end{array} \\right)\n",
    "$\n",
    "\n",
    "where each cell $w_{ij}$ contains a value that represents the degree of spatial contact or interaction between observations $i$ and $j$. A fundamental concept in this context is that of *neighbor* and *neighborhood*. By convention, elements in the diagonal ($w_{ii}$) are set to zero. A *neighbor* of a given observation $i$ is another observation with which $i$ has some degree of connection. In terms of $W$, $i$ and $j$ are thus neighbors if $w_{ij} > 0$. Following this logic, the neighborhood of $i$ will be the set of observations in the system with which it has certain connection, or those observations with a weight greater than zero.\n",
    "\n",
    "There are several ways to create such matrices, and many more to transform them so they contain an accurate representation that aligns with the way we understand spatial interactions between the elements of a system. In this session, we will introduce the most commonly used ones and will show how to compute them with `PySAL`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f6e748-0342-4035-920c-9b712f14c0b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data\n",
    "\n",
    "For this session, we will use the results of the 2016 referendum vote to leave the EU, at the local authority level. In particular, we will focus on the spatial distribution of the vote to Leave, which ended up winning. From a technical point of view, you will be working with polygons which have a value (the percentage of the electorate that voted to Leave the EU) attached to them.\n",
    "\n",
    "All the necessary data have been assembled for convenience in a single file that contains geographic information about each local authority in England, Wales and Scotland, as well as the vote attributes. The file is in the geospatial format [GeoPackage](http://www.geopackage.org/), which presents several advantages over the more traditional shapefile (chief among them, the need of a single file instead of several). The file is available as a download from the course website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f78c9-c3ab-40d1-9a6e-002d56e9ec78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the file in\n",
    "br = gpd.read_file(\"files/brexit.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bbfb4b-f5da-4b4b-8f35-e3f749f4a962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "br.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a804e20d-f934-4081-941d-5db5df101bd8",
   "metadata": {},
   "source": [
    "Now let's index it on the local authority IDs, while keeping those as a column too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc438b-7489-49fc-951e-2ff35432dcfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Index table on the LAD ID\n",
    "br = br.set_index(\"lad16cd\", drop=False)\n",
    "# Display summary\n",
    "br.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504767fc-7faf-4b6f-afa4-ca18d5c08831",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6018af-e722-44f5-a230-90bbf155a2cf",
   "metadata": {},
   "source": [
    "Let's get a first view of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccbb537-0746-406f-a27b-85dd5072d0e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot polygons\n",
    "ax = br.plot(alpha=0.5, color='red');\n",
    "# Add background map, expressing target CRS so the basemap can be reprojected\n",
    "cx.add_basemap(ax, crs=br.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b58f75-1172-429d-a2d9-6b3817e0f511",
   "metadata": {},
   "source": [
    "### Spatial weights matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a71dff-68e9-47a5-985e-474d75444c54",
   "metadata": {},
   "source": [
    "In this session we will be learning the ins and outs of one of the key pieces in spatial analysis: spatial weights matrices. These are structured sets of numbers that formalize geographical relationships between the observations in a dataset. Essentially, a spatial weights matrix of a given geography is a positive definite matrix of dimensions $N$ by $N$, where $N$ is the total number of observations:\n",
    "\n",
    "$\n",
    "W = \\left(\\begin{array}{cccc}\n",
    "0 & w_{12} & \\dots & w_{1N} \\\\\n",
    "w_{21} & \\ddots & w_{ij} & \\vdots \\\\\n",
    "\\vdots & w_{ji} & 0 & \\vdots \\\\\n",
    "w_{N1} & \\dots & \\dots & 0 \n",
    "\\end{array} \\right)\n",
    "$\n",
    "\n",
    "where each cell $w_{ij}$ contains a value that represents the degree of spatial contact or interaction between observations $i$ and $j$. A fundamental concept in this context is that of *neighbor* and *neighborhood*. By convention, elements in the diagonal ($w_{ii}$) are set to zero. A *neighbor* of a given observation $i$ is another observation with which $i$ has some degree of connection. In terms of $W$, $i$ and $j$ are thus neighbors if $w_{ij} > 0$. Following this logic, the neighborhood of $i$ will be the set of observations in the system with which it has certain connection, or those observations with a weight greater than zero.\n",
    "\n",
    "There are several ways to create such matrices, and many more to transform them so they contain an accurate representation that aligns with the way we understand spatial interactions between the elements of a system. In this session, we will introduce the most commonly used ones and will show how to compute them with `PySAL`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c3973b-d275-482e-bca4-d96067d25b72",
   "metadata": {},
   "source": [
    "For this example, we will show how to build a queen contiguity matrix, which considers two observations as neighbors if they share at least one point of their boundary. In other words, for a pair of local authorities in the dataset to be considered neighbours under this $W$, they will need to be sharing border or, in other words, \"touching\" each other to some degree.\n",
    "\n",
    "Technically speaking, we will approach building the contiguity matrix in the same way we did last time. We will begin with a `GeoDataFrame` and pass it on to the queen contiguity weights builder in `PySAL` (`ps.weights.Queen.from_dataframe`). We will also make sure our table of data is previously indexed on the local authority code, so the $W$ is also indexed on that form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4929ae-62a2-494c-b826-2baded0d36e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the spatial weights matrix\n",
    "w = weights.Queen.from_dataframe(br, idVariable=\"lad16cd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0275c1-b4c0-4846-809f-fc44139e84cc",
   "metadata": {},
   "source": [
    "The command above creates an object `w` of the class `W`. This is the format in which spatial weights matrices are stored in `PySAL`. By default, the weights builder (`Queen.from_dataframe`) will use the index of the table, which is useful so we can keep everything in line easily. There is a warning message that we will need to consider in our analysis later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095baf3-1f4a-4d58-a114-139f78a3d383",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualizing spatial weights (Connectivity graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5e4b8-6e1b-43df-8816-4c15b56a6a05",
   "metadata": {
    "tags": []
   },
   "source": [
    "It is useful to visualize the weights, using the `libpysal` method `plot_spatial_weights`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2780184-4ea4-4631-9c1f-96de9dcda501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_spatial_weights(w, br.to_crs(epsg=27700), indexed_on=\"lad16cd\"); # Use the British National Grid, 27700"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad59acc-b9b9-401e-aa88-71f601a42b50",
   "metadata": {},
   "source": [
    "A `W` object can be queried to find out about the contiguity relations it contains. For example, if we would like to know who is a neighbor of observation `E08000012`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55bcdbf-f827-4617-9168-eb8e90e2a021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w['E08000012']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5893a559-0012-4911-887e-10cac06ac335",
   "metadata": {},
   "source": [
    "This returns a Python dictionary that contains the ID codes of each neighbor as keys, and the weights they are assigned as values. Since we are looking at a raw queen contiguity matrix, every neighbor gets a weight of one. If we want to access the weight of a specific neighbor, `E01006691` for example, we can do recursive querying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326fa6b-ddcd-4f3b-9a64-1e06a0ee9c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w['E08000012']['E08000011']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d5efa-5f33-4f86-9735-f29caedd4b3b",
   "metadata": {},
   "source": [
    "`W` objects also have a direct way to provide a list of all the neighbors or their weights for a given observation. This is thanks to the `neighbors` and `weights` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac0b3b7-13d2-4590-bb52-af3842c7644e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w.neighbors['E08000012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d27cc-637f-425c-bda5-74853531f026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w.weights['E08000012']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f703444c-f25a-40ca-aa08-fa66e8b18297",
   "metadata": {},
   "source": [
    "Once created, `W` objects can provide much information about the matrix, beyond the basic attributes one would expect. We have direct access to the number of neighbors each observation has via the attribute `cardinalities`. For example, to find out how many neighbors observation `E01006524` has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb27b6-8090-44a8-943e-01605f70ffd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w.cardinalities['E08000012']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc7d535-e93a-4320-bd47-43b495c73cf9",
   "metadata": {},
   "source": [
    "Since `cardinalities` is a dictionary, it is direct to convert it into a `Series` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69128f91-2a1f-4843-81b7-c781bfbe819b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queen_card = pd.Series(w.cardinalities)\n",
    "queen_card.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d88ba-8dd7-4eb3-8405-c153431fbc9d",
   "metadata": {},
   "source": [
    "This allows, for example, to access quick plotting, which comes in very handy to get an overview of the size of neighborhoods in general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd8f0f-ba2a-4b73-ab6b-d1534de86485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = sns.displot(queen_card, bins=13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d560dbf-8c71-49e0-bc3d-307a1b003b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of observations\n",
    "w.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03957d09-a2fd-4432-a80d-719c05d6d61c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Average number of neighbors\n",
    "w.mean_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbce81-9c73-4204-9bac-8e3c5432d66f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Min number of neighbors\n",
    "w.min_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68976f32-7d26-44e4-a206-8c54904bbd79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Max number of neighbors\n",
    "w.max_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f756bd8d-a2b2-4d19-8527-4d074fb2ec45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Order of IDs (first five only in this case)\n",
    "w.id_order[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721fcd8c-2f64-47fc-ab1e-28f95a7b6c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Islands (observations disconnected)\n",
    "w.islands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d371f6a-b68e-45e3-b154-135721544636",
   "metadata": {},
   "source": [
    "Note how we have \"islands\". Remember these are islands not necessarily in the geographic sense (although some of them will be), but in the mathematical sense of the term: local authorities that are not sharing border with any other one and thus do not have any neighbors. We can inspect and map them to get a better sense of what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106ca2a-7a8f-456d-9517-8871c3f79164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = br.plot(color='k', figsize=(9, 9))\n",
    "br.loc[w.islands, :].plot(color='red', ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c9d45c-4f9d-4b00-8361-523c97cd79f7",
   "metadata": {},
   "source": [
    "In this case, all the islands are indeed \"real\" islands. These cases can create issues in the analysis and distort the results. There are several solutions to this situation such as connecting the islands to other observations through a different criterium (e.g. nearest neighbor), and then combining both spatial weights matrices. For convenience, we will remove them from the dataset because they are a small sample and their removal is likely not to have a large impact in the calculations.\n",
    "\n",
    "Technically, this amounts to a subsetting, for which we will use the `drop` command, which comes in very handy in these cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d463f5-511e-4f87-893b-a4d8471af42e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "br = br.drop(w.islands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82d888-3252-4f4c-bfe5-90507bc61efa",
   "metadata": {},
   "source": [
    "Once we have the set of local authorities that are not an island, we need to re-calculate the weights matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd1b4a-4d0b-406a-8814-b5e4dd1573a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = weights.Queen.from_dataframe(br, idVariable=\"lad16cd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d318b-6879-4261-973e-902875b17fb6",
   "metadata": {},
   "source": [
    "And, finally, let us row-standardize it to make sure every row of the matrix sums up to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1dd28-226d-485c-996f-6cd0d8106810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Row standardize the matrix\n",
    "w.transform = 'R'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1ade70-fd14-4798-8859-63a6b5d81d01",
   "metadata": {},
   "source": [
    "Now, because we have row-standardize them, the weight given to each of the three neighbors is 0.33 which, all together, sum up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f372d-62c8-4cfe-86f2-a3586645bea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w['E08000012']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd013612-5ca7-4c84-a634-b86271cee18c",
   "metadata": {
    "tags": []
   },
   "source": [
    "If we re-plot the connectivity graph now, the islands are gone and we have one connected component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ed89c-81ed-4332-803f-80bf91345a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_spatial_weights(w, br.to_crs(epsg=27700), indexed_on=\"lad16cd\"); # Use the British National Grid, 27700"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e01aa5-5d12-4b0f-9afb-d85c137da680",
   "metadata": {
    "tags": []
   },
   "source": [
    "*Back to presentation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84bbf29-7cad-4ff0-b0f3-00aaa6c7be96",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6bec66-be36-4843-9af6-cff2a5dfd10a",
   "metadata": {},
   "source": [
    "### Spatial lag\n",
    "\n",
    "One of the most direct applications of spatial weight matrices is the so-called *spatial lag*. The spatial lag of a given variable is the product of a spatial weight matrix and the variable itself:\n",
    "\n",
    "$$\n",
    "Y_{sl} = W Y\n",
    "$$\n",
    "\n",
    "where $Y$ is a Nx1 vector with the values of the variable. Recall that the product of a matrix and a vector equals the sum of a row by column element multiplication for the resulting value of a given row. In terms of the spatial lag:\n",
    "\n",
    "$$\n",
    "y_{sl-i} = \\displaystyle \\sum_j w_{ij} y_j\n",
    "$$\n",
    "\n",
    "Since we are using row-standardized weights, $w_{ij}$ becomes a proportion between zero and one, and $y_{sl-i}$ can be seen as the average value of $Y$ in the neighborhood of $i$.\n",
    "\n",
    "Now that we have the data and the spatial weights matrix ready, we can start by computing the spatial lag of the percentage of votes that went to leave the EU. We can calculate the spatial lag for the variable `Pct_Leave` and store it directly in the main table with the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb94a4-f303-4b48-9131-b098ae38006f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "br['w_Pct_Leave'] = weights.lag_spatial(w, br['Pct_Leave'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b21b9b-7ad6-461d-bfcf-ff85a65f4f61",
   "metadata": {},
   "source": [
    "Let us have a quick look at the resulting variable, as compared to the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c33652-3581-4c29-9a4a-9033b43e2a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "br[['lad16cd', 'Pct_Leave', 'w_Pct_Leave']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c912098c-404d-410c-9462-c889d6f7a411",
   "metadata": {},
   "source": [
    "The way to interpret the spatial lag (`w_Pct_Leave`) for say the first observation is as follow: Hartlepool, where 69,6% of the electorate voted to leave is surrounded by neighbouring local authorities where, on average, almost 60% of the electorate also voted to leave the EU. For the purpose of illustration, we can in fact check this is correct by querying the spatial weights matrix to find out Hartepool's neighbors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e2191-6026-4a69-af96-75f30e987e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w.neighbors['E06000001']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ec734-401f-4955-af4f-a8904fb6e415",
   "metadata": {},
   "source": [
    "And then checking their values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28536358-d37f-4525-b423-129d35494189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neis = br.loc[w.neighbors['E06000001'], 'Pct_Leave']\n",
    "neis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0107a4fe-ec1b-450c-a40b-8ed0b270c5f2",
   "metadata": {},
   "source": [
    "And the average value, which we saw in the spatial lag is 61.8, can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd37da-a8ae-47b8-8be4-dc8652ad67a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neis.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50409895-6702-4884-b747-0ca708393c7b",
   "metadata": {},
   "source": [
    "For some of the techniques we will be seeing below, it makes more sense to operate with the standardized version of a variable, rather than with the raw one. Standardizing means to substract the average value and divide by the standard deviation each observation of the column. This can be done easily with a bit of basic algebra in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf41503-ccb6-460d-85d0-a196a60cdafb",
   "metadata": {},
   "source": [
    "In order to easily compare different scatter plots and spot outlier observations, it is common practice to standardize the values of the variable before computing its spatial lag and plotting it. This can be accomplished by substracting the average value and dividing the result by the standard deviation:\n",
    "\n",
    "$$\n",
    "z_i = \\dfrac{y - \\bar{y}}{\\sigma_y}\n",
    "$$\n",
    "\n",
    "where $z_i$ is the standardized version of $y_i$, $\\bar{y}$ is the average of the variable, and $\\sigma$ its standard deviation.\n",
    "\n",
    "Creating a standardized Moran Plot implies that average values are centered in the plot (as they are zero when standardized) and dispersion is expressed in standard deviations, with the rule of thumb of values greater or smaller than two standard deviations being *outliers*. A standardized Moran Plot also partitions the space into four quadrants that represent different situations:\n",
    "\n",
    "1. High-High (*HH*): values above average surrounded by values above average.\n",
    "1. Low-Low (*LL*): values below average surrounded by values below average.\n",
    "1. High-Low (*HL*): values above average surrounded by values below average.\n",
    "1. Low-High (*LH*): values below average surrounded by values above average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9b68b-9e84-42c2-a4cf-6127e0882a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "br['Pct_Leave_std'] = (\n",
    "    br['Pct_Leave'] - br['Pct_Leave'].mean()\n",
    ") / br['Pct_Leave'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b9023-32d7-47e0-a5eb-741e26cb8b0d",
   "metadata": {},
   "source": [
    "Finally, to be able to explore the spatial patterns of the standardized values, also called sometimes $z$ values, we need to create its spatial lag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85806c2b-8b73-40ac-8f1a-f5246ad22a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "br['w_Pct_Leave_std'] = weights.lag_spatial(w, br['Pct_Leave_std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbfee2e-3462-4e12-a1fc-9b3257224a22",
   "metadata": {},
   "source": [
    "## Global Spatial autocorrelation\n",
    "\n",
    "Global spatial autocorrelation relates to the overall geographical pattern present in the data. Statistics designed to measure this trend thus characterize a map in terms of its degree of clustering and summarize it. This summary can be visual or numerical. In this section, we will walk through an example of each of them: the Moran Plot, and Moran's I statistic of spatial autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c0b4a-aa42-4d9f-b4cc-58516ddb5fc1",
   "metadata": {},
   "source": [
    "### Moran Plot\n",
    "\n",
    "The moran plot is a way of visualizing a spatial dataset to explore the nature and strength of spatial autocorrelation. It is essentially a traditional scatter plot in which the variable of interest is displayed against its spatial lag. In order to be able to interpret values as above or below the mean, and their quantities in terms of standard deviations, the variable of interest is usually standardized by substracting its mean and dividing it by its standard deviation.\n",
    "\n",
    "Technically speaking, creating a Moran Plot is very similar to creating any other scatter plot in Python, provided we have standardized the variable and calculated its spatial lag beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf4419-b0c2-49bc-8362-8cb3c387f80a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Plot values\n",
    "sns.regplot(x='Pct_Leave_std', y='w_Pct_Leave_std', data=br, ci=None)\n",
    "# Add vertical and horizontal lines\n",
    "plt.axvline(0, c='k', alpha=0.5)\n",
    "plt.axhline(0, c='k', alpha=0.5)\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f66d2a3-6229-4abe-a8dc-7954fde87913",
   "metadata": {},
   "source": [
    "The figure above displays the relationship between the standardized percentage which voted to Leave the EU (`Pct_Leave_std`) and its spatial lag which, because the $W$ that was used is row-standardized, can be interpreted as the average percentage which voted to Leave in the surrounding areas of a given Local Authority. In order to guide the interpretation of the plot, a linear fit is also included in the post. This line represents the best linear fit to the scatter plot or, in other words, what is the best way to represent the relationship between the two variables as a straight line. \n",
    "\n",
    "The plot displays a positive relationship between both variables. This is associated with the presence of *positive* spatial autocorrelation: similar values tend to be located close to each other. This means that the *overall trend* is for high values to be close to other high values, and for low values to be surrounded by other low values. This however does not mean that this is only situation in the dataset: there can of course be particular cases where high values are surrounded by low ones, and viceversa. But it means that, if we had to summarize the main pattern of the data in terms of how clustered similar values are, the best way would be to say they are positively correlated and, hence, clustered over space.\n",
    "\n",
    "In the context of the example, this can be interpreted along the lines of: local authorities display positive spatial autocorrelation in the way they voted in the EU referendum. This means that local authorities with high percentage of Leave voters tend to be located nearby other local authorities where a significant share of the electorate also voted to Leave, and viceversa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c18865-a552-44fd-9d20-ed1848d4d4b7",
   "metadata": {},
   "source": [
    "### Moran's I\n",
    "\n",
    "The Moran Plot is an excellent tool to explore the data and get a good sense of how much values are clustered over space. However, because it is a graphical device, it is sometimes hard to condense its insights into a more concise way. For these cases, a good approach is to come up with a statistical measure that summarizes the figure. This is exactly what Moran's I is meant to do. \n",
    "\n",
    "Very much in the same way the mean summarizes a crucial element of the distribution of values in a non-spatial setting, so does Moran's I for a spatial dataset. Continuing the comparison, we can think of the mean as a single numerical value summarizing a histogram or a kernel density plot. Similarly, Moran's I captures much of the essence of the Moran Plot. In fact, there is an even close connection between the two: the value of Moran's I corresponds with the slope of the linear fit overlayed on top of the Moran Plot.\n",
    "\n",
    "In order to calculate Moran's I in our dataset, we can call a specific function in `PySAL` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217ecb1-3691-44ed-8f4b-af97106380ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mi = esda.Moran(br['Pct_Leave'], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625e663-d404-4d82-825b-397419b36668",
   "metadata": {},
   "source": [
    "Note how we do not need to use the standardized version in this context as we will not represent it visually.\n",
    "\n",
    "The method `ps.Moran` creates an object that contains much more information than the actual statistic. If we want to retrieve the value of the statistic, we can do it this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e0f023-4735-474b-843b-9432c90ca452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mi.I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf25458a-14a4-4ee1-a294-5c634dd2db10",
   "metadata": {},
   "source": [
    "The other bit of information we will extract from Moran's I relates to statistical inference: how likely is the pattern we observe in the map and Moran's I captures in its value to be generated by an entirely random process? If we considered the same variable but shuffled its locations randomly, would we obtain a map with similar characteristics?\n",
    "\n",
    "The specific details of the mechanism to calculate this are beyond the scope of the session, but it is important to know that a small enough p-value associated with the Moran's I of a map allows to reject the hypothesis that the map is random. In other words, we can conclude that the map displays more spatial pattern that we would expect if the values had been randomly allocated to a particular location.\n",
    "\n",
    "The most reliable p-value for Moran's I can be found in the attribute `p_sim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4a626-21e9-4074-b1eb-53e0a3146478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mi.p_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863efcf7-9c69-41fd-a31e-02a69d239c09",
   "metadata": {},
   "source": [
    "That is just 0.1% and, by standard terms, it would be considered statistically significant. We can quickly ellaborate on its intuition. What that 0.001 (or 0.1%) means is that, if we generated a large number of maps with the same values but randomly allocated over space, and calculated the Moran's I statistic for each of those maps, only 0.1% of them would display a larger (absolute) value than the one we obtain from the real data, and the other 99.9% of the random maps would receive a smaller (absolute) value of Moran's I. If we remember again that the value of Moran's I can also be interpreted as the slope of the Moran Plot, what we have is that, in this case, the particular spatial arrangement of values for the Leave votes is more concentrated than if the values had been allocated following a completely spatially random process, hence the statistical significance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2168d9-d26b-4e5b-8c23-f8cfc959d7fe",
   "metadata": {},
   "source": [
    "Once we have calculated Moran's I and created an object like `mi`, we can use some of the functionality in `splot` to replicate the plot above more easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af68aec5-558c-4223-98a7-b512934394a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moran_scatterplot(mi);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a01e46-87d0-4c37-beb7-894936350337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_moran(mi);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f61b857-be13-4239-a90c-e511c7627bd7",
   "metadata": {},
   "source": [
    "As a first step, the global autocorrelation analysis can teach us that observations do seem to be positively correlated over space. In terms of our initial goal to find spatial structure in the attitude towards Brexit, this view seems to align: if the vote had no such structure, it should not show a pattern over space -technically, it would show a random one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0cc84-4dd2-4f53-885f-20dd61361d99",
   "metadata": {},
   "source": [
    "## Local Spatial autocorrelation\n",
    "\n",
    "Moran's I is good tool to summarize a dataset into a single value that informs about its degree of *clustering*. However, it is not an appropriate measure to identify areas within the map where specific values are located. In other words, Moran's I can tell us values are clustered overall, but it will not inform us about *where* the clusters are. For that purpose, we need to use a *local* measure of spatial autocorrelation. Local measures consider each single observation in a dataset and operate on them, as oposed to on the overall data, as *global* measures do. Because of that, they are not good a summarizing a map, but they allow to obtain further insight.\n",
    "\n",
    "In this session, we will consider [Local Indicators of Spatial Association](http://onlinelibrary.wiley.com/doi/10.1111/j.1538-4632.1995.tb00338.x/abstract) (LISAs), a local counter-part of global measures like Moran's I. At the core of these method is a classification of the observations in a dataset into four groups derived from the Moran Plot: high values surrounded by high values (HH), low values nearby other low values (LL), high values among low values (HL), and viceversa (LH). Each of these groups are typically called \"quadrants\". An illustration of where each of these groups fall into the Moran Plot can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ee23d-bdbb-4292-a17b-751c2b7067d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Plot values\n",
    "sns.regplot(x='Pct_Leave_std', y='w_Pct_Leave_std', data=br, ci=None)\n",
    "# Add vertical and horizontal lines\n",
    "plt.axvline(0, c='k', alpha=0.5)\n",
    "plt.axhline(0, c='k', alpha=0.5)\n",
    "plt.text(1.75, 0.5, \"HH\", fontsize=25)\n",
    "plt.text(1.5, -1.5, \"HL\", fontsize=25)\n",
    "plt.text(-2, 1, \"LH\", fontsize=25)\n",
    "plt.text(-1.5, -2.5, \"LL\", fontsize=25)\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ca3f3-966e-4208-a306-6eb8c459e187",
   "metadata": {},
   "source": [
    "So far we have classified each observation in the dataset depending on its value and that of its neighbors. This is only half way into identifying areas of unusual concentration of values. To know whether each of the locations is a *statistically significant* cluster of a given kind, we again need to compare it with what we would expect if the data were allocated in a completely random way. After all, by definition, every observation will be of one kind of another, based on the comparison above. However, what we are interested in is whether the strength with which the values are concentrated is unusually high. \n",
    "\n",
    "This is exactly what LISAs are designed to do. As before, a more detailed description of their statistical underpinnings is beyond the scope in this context, but we will try to shed some light into the intuition of how they go about it. The core idea is to identify cases in which the comparison between the value of an observation and the average of its neighbors is either more similar (HH, LL) or dissimilar (HL, LH) than we would expect from pure chance. The mechanism to do this is similar to the one in the global Moran's I, but applied in this case to each observation, resulting then in as many statistics as original observations.\n",
    "\n",
    "LISAs are widely used in many fields to identify clusters of values in space. They are a very useful tool that can quickly return areas in which values are concentrated and provide *suggestive* evidence about the processes that might be at work. For that, they have a prime place in the exploratory toolbox. Examples of contexts where LISAs can be useful include: identification of spatial clusters of poverty in regions, detection of ethnic enclaves, delineation of areas of particularly high/low activity of any phenomenon, etc.\n",
    "\n",
    "In Python, we can calculate LISAs in a very streamlined way thanks to `PySAL`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a894da-60c3-40e9-b0fd-893b31a5cd36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lisa = esda.Moran_Local(br['Pct_Leave'], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8b693-3e24-4af3-8ef6-b99d83b1e6e3",
   "metadata": {},
   "source": [
    "All we need to pass is the variable of interest -percentage of Leave votes- and the spatial weights that describes the neighborhood relations between the different observation that make up the dataset.\n",
    "\n",
    "Because of their very nature, looking at the numerical result of LISAs is not always the most useful way to exploit all the information they can provide. Remember that we are calculating a statistic for every sigle observation in the data so, if we have many of them, it will be difficult to extract any meaningful pattern. Instead, what is typically done is to create a map, a cluster map as it is usually called, that extracts the significant observations (those that are highly unlikely to have come from pure chance) and plots them with a specific color depending on their quadrant category.\n",
    "\n",
    "All of the needed pieces are contained inside the `lisa` object we have created above. But, to make the map making more straightforward, it is convenient to pull them out and insert them in the main data table, `br`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2798c-b843-424c-98ea-9f323f9fa31d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Break observations into significant or not\n",
    "br['significant'] = lisa.p_sim < 0.05\n",
    "# Store the quadrant they belong to\n",
    "br['quadrant'] = lisa.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb045ee-1fa6-4280-880b-ab8e3e8561a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "br['quadrant']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b64e3a-2f80-44ae-bbb8-88d787f9083c",
   "metadata": {},
   "source": [
    "Let us stop for second on these two steps. First, the `significant` column. Similarly as with global Moran's I, `PySAL` is automatically computing a p-value for each LISA. Because not every observation represents a statistically significant one, we want to identify those with a p-value small enough that rules out the possibility of obtaining a similar situation from pure chance. Following a similar reasoning as with global Moran's I, we select 5% as the threshold for statistical significance. To identify these values, we create a variable, `significant`, that contains `True` if the p-value of the observation is satisfies the condition, and `False` otherwise. We can check this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79badef-3b1b-48bc-88af-8e5c79de9aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "br['significant'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3c016-57db-4b85-b9c6-77771c469695",
   "metadata": {},
   "source": [
    "And the first five p-values can be checked by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4c87f-215b-47e4-9a6a-8301886dbb42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lisa.p_sim[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ed5ec-8d6a-42fc-b977-180616ad7dbc",
   "metadata": {},
   "source": [
    "Note how the third and fourth are smaller than 0.05, as the variable `significant` correctly identified.\n",
    "\n",
    "Second, the quadrant each observation belongs to. This one is easier as it comes built into the `lisa` object directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9a679-598a-4780-a0d5-57ae4ee306e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "br['quadrant'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb088e66-4228-453e-8ef6-42edec78b000",
   "metadata": {},
   "source": [
    "The correspondence between the numbers in the variable and the actual quadrants is as follows:\n",
    "\n",
    "* 1: HH\n",
    "* 2: LH\n",
    "* 3: LL\n",
    "* 4: HL\n",
    "\n",
    "With these two elements, `significant` and `quadrant`, we can build a typical LISA cluster map combining the mapping skills with what we have learned about subsetting and querying tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3608057d-a2b5-4ef8-b3e9-02d1f039f336",
   "metadata": {},
   "source": [
    "We can create a quick LISA cluster map with `splot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44d110-3f38-492f-8ae9-0b1f78c1996b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lisa_cluster(lisa, br);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b00c68-e228-4cce-9377-062ffbdddf3f",
   "metadata": {},
   "source": [
    "Or, if we want to have more control over what is being displayed, and how each component is presented, we can \"cook\" the plot ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a49ac-110d-4ede-9242-0337d3047b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "# Plot insignificant clusters\n",
    "ns = br.loc[br['significant']==False, 'geometry']\n",
    "ns.plot(ax=ax, color='k')\n",
    "# Plot HH clusters\n",
    "hh = br.loc[(br['quadrant']==1) & (br['significant']==True), 'geometry']\n",
    "hh.plot(ax=ax, color='red')\n",
    "# Plot LL clusters\n",
    "ll = br.loc[(br['quadrant']==3) & (br['significant']==True), 'geometry']\n",
    "ll.plot(ax=ax, color='blue')\n",
    "# Plot LH clusters\n",
    "lh = br.loc[(br['quadrant']==2) & (br['significant']==True), 'geometry']\n",
    "lh.plot(ax=ax, color='#83cef4')\n",
    "# Plot HL clusters\n",
    "hl = br.loc[(br['quadrant']==4) & (br['significant']==True), 'geometry']\n",
    "hl.plot(ax=ax, color='#e59696')\n",
    "# Style and draw\n",
    "f.suptitle('LISA for Brexit vote', size=30)\n",
    "f.set_facecolor('0.75')\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69c5ede-9402-4b2b-bc1e-93fd6d957df3",
   "metadata": {},
   "source": [
    "The map above displays the LISA results of the Brexit vote. In bright red, we find those local authorities with an unusual concentration of high Leave voters surrounded also by high levels of Leave vote. This corresponds with areas in the East of England, the Black Country, and East of London. In light red, we find the first type of *spatial outliers*. These are areas with high Leave vote but surrounded by areas with low support for leaving the EU (e.g. central London). Finally, in light blue we find the other type of spatial outlier: local authorities with low Leave support surrounded by other authorities with high support.\n",
    "\n",
    "The substantive interpretation of a LISA map needs to relate its output to the original intention of the analyst who created the map. In this case, our original idea was to explore the spatial structure of support to leaving the EU. The LISA proves a fairly useful tool in this context. Comparing the LISA map above with the choropleth we started with, we can interpret the LISA as \"simplification\" of the detailed but perhaps too complicated picture in the choropleth that focuses the reader's attention to the areas that display a particularly high concentration of (dis)similar values, helping the spatial structure of the vote emerge in a more explicit way. The result of this highlights the relevance that the East of England and the Midlands had in voting to Leave, as well as the regions of the map where there was a lot less excitement about Leaving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14b7f5-78f5-4fef-8f31-59c3bbfe4c3b",
   "metadata": {},
   "source": [
    "The results from the LISA statistics can be connected to the Moran plot to visualise where in the scatter plot each type of polygon falls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caea839-8614-4064-bef6-10c15ba58ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_local_autocorrelation(lisa, br, 'Pct_Leave');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab701284-0bf4-4e3b-8c59-d20d4d1e4d69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# FURTHER MATERIALS: Choropleth maps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
